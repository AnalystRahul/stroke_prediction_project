plot(roc_curve_rf, col = "green")
ggsave("visualizations/random_forest_roc_curve.png", plot = last_plot())
# Save models
saveRDS(model_logistic, "models/model_logistic.rds")
saveRDS(model_rf, "models/model_rf.rds")
# hyperparameter_tuning.R
# Install necessary packages
if (!require(caret)) install.packages("caret")
if (!require(randomForest)) install.packages("randomForest")
# Load required libraries
library(tidyverse)
library(caret)
library(randomForest)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the preprocessed data
data <- read.csv("data/preprocessed_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level))
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
# Ensure factor columns are treated as factors
data$stroke <- as.factor(data$stroke)
data$gender <- as.factor(data$gender)
data$ever_married <- as.factor(data$ever_married)
data$work_type <- as.factor(data$work_type)
data$Residence_type <- as.factor(data$Residence_type)
data$smoking_status <- as.factor(data$smoking_status)
data$bmi_category <- as.factor(data$bmi_category)
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Ensure consistent factor levels in test data
for (col in names(train_data)) {
  if (is.factor(train_data[[col]])) {
    test_data[[col]] <- factor(test_data[[col]], levels = levels(train_data[[col]]))
  }
}
# Define the parameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5)
)
# Train the model using cross-validation
set.seed(123)
control <- trainControl(method = "cv", number = 5, search = "grid")
model_rf_tuned <- train(
  stroke ~ ., data = train_data,
  method = "rf",
  trControl = control,
  tuneGrid = tune_grid
)
# Print the best model
print(model_rf_tuned$bestTune)
# Make predictions and evaluate the tuned model
predictions_rf_tuned <- predict(model_rf_tuned, newdata = test_data)
confusionMatrix(predictions_rf_tuned, test_data$stroke)
# Save the tuned model
saveRDS(model_rf_tuned, "
# feature_engineering.R
# Install necessary packages
if (!require(tidyverse)) install.packages("tidyverse")
# Load required libraries
library(tidyverse)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the preprocessed data
data <- read.csv("data/preprocessed_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level) & !is.na(hypertension))
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
data$hypertension <- as.numeric(data$hypertension)
# Add interaction terms
data <- data %>%
  mutate(
    age_bmi_interaction = age * bmi,
    age_hypertension_interaction = age * hypertension,
    bmi_hypertension_interaction = bmi * hypertension
  )
# Save the engineered data
write.csv(data, "data/engineered_data.csv", row.names = FALSE)
# gradient_boosting.R
# Install necessary packages
if (!require(xgboost)) install.packages("xgboost")
# Load required libraries
library(tidyverse)
library(caret)
library(xgboost)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the engineered data
data <- read.csv("data/engineered_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level))
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Convert data to matrix format for xgboost
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -c(1, 12)]), label = train_data$stroke)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -c(1, 12)]), label = test_data$stroke)
# Train the xgboost model
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.1,
  nthread = 2,
  nrounds = 100
)
xgb_model <- xgb.train(params, train_matrix, params$nrounds)
# Make predictions
predictions <- predict(xgb_model, test_matrix)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Evaluate the model
confusionMatrix(factor(predicted_classes), test_data$stroke)
roc_curve <- roc(test_data$stroke, predictions)
auc(roc_curve)
# Save the model
xgb.save(xgb_model, "models/xgb_model.model")
# gradient_boosting.R
# Install necessary packages
if (!require(xgboost)) install.packages("xgboost")
# Load required libraries
library(tidyverse)
library(caret)
library(xgboost)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the engineered data
data <- read.csv("data/engineered_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level))
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Convert data to matrix format for xgboost
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -c(1, 12)]), label = train_data$stroke)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -c(1, 12)]), label = test_data$stroke)
# Train the xgboost model
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.1,
  nthread = 2,
  nrounds = 100
)
xgb_model <- xgb.train(params, train_matrix, params$nrounds)
# Make predictions
predictions <- predict(xgb_model, test_matrix)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Evaluate the model
confusionMatrix(factor(predicted_classes), test_data$stroke)
roc_curve <- roc(test_data$stroke, predictions)
auc(roc_curve)
# Save the model
xgb.save(xgb_model, "models/xgb_model.model")
if (!require(tidyverse)) install.packages("tidyverse")
if (!require(skimr)) install.packages("skimr")
if (!require(mice)) install.packages("mice")
if (!require(caTools)) install.packages("caTools")
# Load required libraries
library(tidyverse)
library(skimr)
library(mice)
library(caTools)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the dataset
data <- read.csv("data/healthcare-dataset-stroke-data.csv")
# Check for missing values
skim(data)
# Handle missing values using multiple imputation
imputed_data <- mice(data, m = 5, method = 'pmm', maxit = 50, seed = 500)
data <- complete(imputed_data, 1)
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
# Ensure factor columns are treated as factors
data$stroke <- as.factor(data$stroke)
data$gender <- as.factor(data$gender)
data$ever_married <- as.factor(data$ever_married)
data$work_type <- as.factor(data$work_type)
data$Residence_type <- as.factor(data$Residence_type)
data$smoking_status <- as.factor(data$smoking_status)
# Save the preprocessed data
write.csv(data, "data/preprocessed_data.csv", row.names = FALSE)
# Install necessary packages
if (!require(ggplot2)) install.packages("ggplot2")
# Load required libraries
library(tidyverse)
library(skimr)
library(ggplot2)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the preprocessed data
data <- read.csv("data/preprocessed_data.csv")
# Summary statistics
summary(data)
skim(data)
# Visualize the distribution of age
age_dist_plot <- ggplot(data, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age", y = "Frequency")
# Save the plot
ggsave("visualizations/age_distribution.png", plot = age_dist_plot)
# Visualize the distribution of BMI
bmi_dist_plot <- ggplot(data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, fill = "green", color = "black") +
  theme_minimal() +
  labs(title = "BMI Distribution", x = "BMI", y = "Frequency")
# Save the plot
ggsave("visualizations/bmi_distribution.png", plot = bmi_dist_plot)
# Visualize stroke count by gender
stroke_gender_plot <- ggplot(data, aes(x = gender, fill = stroke)) +
  geom_bar(position = "dodge") +
  theme_minimal() +
  labs(title = "Stroke Count by Gender", x = "Gender", y = "Count") +
  scale_fill_manual(values = c("0" = "blue", "1" = "red"))
# Save the plot
ggsave("visualizations/stroke_by_gender.png", plot = stroke_gender_plot)
# Visualize stroke count by BMI category
stroke_bmi_plot <- ggplot(data, aes(x = bmi_category, fill = stroke)) +
  geom_bar(position = "dodge") +
  theme_minimal() +
  labs(title = "Stroke Count by BMI Category", x = "BMI Category", y = "Count") +
  scale_fill_manual(values = c("0" = "blue", "1" = "red"))
# Save the plot
ggsave("visualizations/stroke_by_bmi_category.png", plot = stroke_bmi_plot)
# Correlation heatmap
corr_matrix <- data %>%
  select(age, avg_glucose_level, bmi) %>%
  cor()
# Convert the correlation matrix to a format suitable for ggplot
corr_df <- as.data.frame(as.table(corr_matrix))
# Plot the correlation heatmap
corr_heatmap_plot <- ggplot(corr_df, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = round(Freq, 2))) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap")
# Save the plot
ggsave("visualizations/correlation_heatmap.png", plot = corr_heatmap_plot)
# Install necessary packages
if (!require(caret)) install.packages("caret")
if (!require(randomForest)) install.packages("randomForest")
if (!require(pROC)) install.packages("pROC")
# Load required libraries
library(tidyverse)
library(caret)
library(randomForest)
library(pROC)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the preprocessed data
data <- read.csv("data/preprocessed_data.csv")
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
# Ensure factor columns are treated as factors
data$stroke <- as.factor(data$stroke)
data$gender <- as.factor(data$gender)
data$ever_married <- as.factor(data$ever_married)
data$work_type <- as.factor(data$work_type)
data$Residence_type <- as.factor(data$Residence_type)
data$smoking_status <- as.factor(data$smoking_status)
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Ensure consistent factor levels in test data
for (col in names(train_data)) {
  if (is.factor(train_data[[col]])) {
    test_data[[col]] <- factor(test_data[[col]], levels = levels(train_data[[col]]))
  }
}
# Train a logistic regression model
model_logistic <- glm(stroke ~ ., data = train_data, family = binomial)
# Summary of the model
summary(model_logistic)
# Make predictions and evaluate the logistic regression model
predictions <- predict(model_logistic, newdata = test_data, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Convert predicted classes to factors with the same levels as the actual data
predicted_classes <- factor(predicted_classes, levels = levels(test_data$stroke))
# Evaluate the model
confusionMatrix(predicted_classes, test_data$stroke)
# Calculate AUC-ROC for logistic regression
roc_curve <- roc(test_data$stroke, predictions)
auc(roc_curve)
# Plot ROC curve for logistic regression
plot(roc_curve, col = "blue")
ggsave("visualizations/logistic_roc_curve.png", plot = last_plot())
# Train a Random Forest model
set.seed(123)
model_rf <- randomForest(stroke ~ ., data = train_data, ntree = 100, mtry = 3, importance = TRUE)
# Summary of the model
print(model_rf)
# Make predictions and evaluate the Random Forest model
predictions_rf <- predict(model_rf, newdata = test_data)
predicted_classes_rf <- factor(predictions_rf > 0.5, levels = c(FALSE, TRUE), labels = levels(test_data$stroke))
# Evaluate the Random Forest model
confusionMatrix(predicted_classes_rf, test_data$stroke)
# Calculate AUC-ROC for Random Forest
roc_curve_rf <- roc(test_data$stroke, as.numeric(predictions_rf))
auc(roc_curve_rf)
# Plot ROC curve for Random Forest
plot(roc_curve_rf, col = "green")
ggsave("visualizations/random_forest_roc_curve.png", plot = last_plot())
# Save models
saveRDS(model_logistic, "models/model_logistic.rds")
saveRDS(model_rf, "models/model_rf.rds")
# hyperparameter_tuning.R
# Install necessary packages
if (!require(caret)) install.packages("caret")
if (!require(randomForest)) install.packages("randomForest")
# Load required libraries
library(tidyverse)
library(caret)
library(randomForest)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the preprocessed data
data <- read.csv("data/preprocessed_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level))
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
# Ensure factor columns are treated as factors
data$stroke <- as.factor(data$stroke)
data$gender <- as.factor(data$gender)
data$ever_married <- as.factor(data$ever_married)
data$work_type <- as.factor(data$work_type)
data$Residence_type <- as.factor(data$Residence_type)
data$smoking_status <- as.factor(data$smoking_status)
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Ensure consistent factor levels in test data
for (col in names(train_data)) {
  if (is.factor(train_data[[col]])) {
    test_data[[col]] <- factor(test_data[[col]], levels = levels(train_data[[col]]))
  }
}
# Define the parameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5)
)
# Train the model using cross-validation
set.seed(123)
control <- trainControl(method = "cv", number = 5, search = "grid")
model_rf_tuned <- train(
  stroke ~ ., data = train_data,
  method = "rf",
  trControl = control,
  tuneGrid = tune_grid
)
# Print the best model
print(model_rf_tuned$bestTune)
# Make predictions and evaluate the tuned model
predictions_rf_tuned <- predict(model_rf_tuned, newdata = test_data)
confusionMatrix(predictions_rf_tuned, test_data$stroke)
# Save the tuned model
saveRDS(model_rf_tuned, "models/model_rf_tuned.rds")
# Install necessary packages
if (!require(tidyverse)) install.packages("tidyverse")
# Load required libraries
library(tidyverse)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the preprocessed data
data <- read.csv("data/preprocessed_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level) & !is.na(hypertension))
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
data$hypertension <- as.numeric(data$hypertension)
# Add interaction terms
data <- data %>%
  mutate(
    age_bmi_interaction = age * bmi,
    age_hypertension_interaction = age * hypertension,
    bmi_hypertension_interaction = bmi * hypertension
  )
# Save the engineered data
write.csv(data, "data/engineered_data.csv", row.names = FALSE)
# Install necessary packages
if (!require(xgboost)) install.packages("xgboost")
# Load required libraries
library(tidyverse)
library(caret)
library(xgboost)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the engineered data
data <- read.csv("data/engineered_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level))
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Convert data to matrix format for xgboost
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -c(1, 12)]), label = train_data$stroke)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -c(1, 12)]), label = test_data$stroke)
# Train the xgboost model
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.1,
  nthread = 2,
  nrounds = 100
)
xgb_model <- xgb.train(params, train_matrix, params$nrounds)
# Make predictions
predictions <- predict(xgb_model, test_matrix)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Evaluate the model
confusionMatrix(factor(predicted_classes), test_data$stroke)
roc_curve <- roc(test_data$stroke, predictions)
auc(roc_curve)
# Save the model
xgb.save(xgb_model, "models/xgb_model.model")
# Install necessary packages
if (!require(xgboost)) install.packages("xgboost")
# Load required libraries
library(tidyverse)
library(caret)
library(xgboost)
# Set the working directory
setwd("C:/Users/rahul/OneDrive/Desktop/Projects/Google Data Analystics/R Project/Stroke Prediction/stroke_prediction_project")
# Load the engineered data
data <- read.csv("data/engineered_data.csv")
# Handle missing values
data <- data %>%
  filter(!is.na(bmi) & !is.na(age) & !is.na(avg_glucose_level))
# Ensure numeric columns are treated as numeric
data$age <- as.numeric(data$age)
data$bmi <- as.numeric(data$bmi)
data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
data$stroke <- as.numeric(as.character(data$stroke))
data$hypertension <- as.numeric(data$hypertension)
data$heart_disease <- as.numeric(data$heart_disease)
# Convert categorical variables to numeric
data$gender <- as.numeric(as.factor(data$gender))
data$ever_married <- as.numeric(as.factor(data$ever_married))
data$work_type <- as.numeric(as.factor(data$work_type))
data$Residence_type <- as.numeric(as.factor(data$Residence_type))
data$smoking_status <- as.numeric(as.factor(data$smoking_status))
# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Convert data to matrix format for xgboost
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -which(names(train_data) == "stroke")]), label = train_data$stroke)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -which(names(test_data) == "stroke")]), label = test_data$stroke)
# Train the xgboost model
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.1,
  nthread = 2,
  nrounds = 100
)
xgb_model <- xgb.train(params, train_matrix, params$nrounds)
# Make predictions
predictions <- predict(xgb_model, test_matrix)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Evaluate the model
confusion_matrix <- confusionMatrix(factor(predicted_classes, levels = c(0, 1)), factor(test_data$stroke, levels = c(0, 1)))
print(confusion_matrix)
roc_curve <- roc(test_data$stroke, predictions)
auc_value <- auc(roc_curve)
print(auc_value)
# Plot ROC curve
plot(roc_curve, col = "green")
ggsave("visualizations/xgboost_roc_curve.png", plot = last_plot())
# Save the model
xgb.save(xgb_model, "models/xgb_model.model")
q()
